{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c68ab395-3816-4aa9-bc97-6734b3aad547",
   "metadata": {},
   "source": [
    "# Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50ca004a-c695-4f6d-9fe2-1dbb35ea07b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16e7856-aec8-48e8-a64f-ea68d625172c",
   "metadata": {},
   "source": [
    "# Define Target Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e555f704-4f90-4009-a4f9-db9168dd1f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./grad_cam_result'):\n",
    "    os.mkdir('grad_cam_result')\n",
    "    \n",
    "target_dir='./grad_cam_result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91ae6ea3-9ae5-44df-b4d1-798b5fbd023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "learning_rate = 1e-4\n",
    "epochs = 1\n",
    "batch_size = 256\n",
    "log_interval = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c63f6a1-48b4-457b-814f-585b3c68f9ef",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b1faf5e-d888-4e47-aeee-94df4fc071eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "627cfc30-581f-49cb-80a2-93a360386fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_train = torchvision.datasets.CIFAR10('./data/CIFAR10', train=True, download=True, transform=transform)\n",
    "cifar_test = torchvision.datasets.CIFAR10('./data/CIFAR10', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "302404f2-3b45-40cb-8edd-2f89d56518d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(dataset=cifar_train, shuffle=True, batch_size=batch_size)\n",
    "test_loader = data.DataLoader(dataset=cifar_test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab82fa6-a0e1-4f67-896e-c1632c72b4b4",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46785c39-18ee-4b56-937d-cdaffe57883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        \n",
    "        self.vgg = torchvision.models.vgg11(pretrained=True)\n",
    "        \n",
    "        self.features_conv = self.vgg.features[:20]\n",
    "        \n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        \n",
    "        self.classifier = self.vgg.classifier\n",
    "        \n",
    "        self.classifier[6] = nn.Linear(in_features=4096, out_features=10, bias=True)\n",
    "        \n",
    "        self.gradients = None\n",
    "    \n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features_conv(x)\n",
    "        \n",
    "        if self.train and x.requires_grad: # only for train, register hook\n",
    "            x.register_hook(self.activations_hook)\n",
    "        \n",
    "        x = self.max_pool(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    def get_activations(self, x):\n",
    "        return self.features_conv(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1de59584-357d-44d0-9c78-70ee1681780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78a71d1b-57c5-4671-a32a-aa149b56b94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (vgg): VGG(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace=True)\n",
      "      (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (9): ReLU(inplace=True)\n",
      "      (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (12): ReLU(inplace=True)\n",
      "      (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (14): ReLU(inplace=True)\n",
      "      (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (17): ReLU(inplace=True)\n",
      "      (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (19): ReLU(inplace=True)\n",
      "      (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Dropout(p=0.5, inplace=False)\n",
      "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (4): ReLU(inplace=True)\n",
      "      (5): Dropout(p=0.5, inplace=False)\n",
      "      (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (features_conv): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): ReLU(inplace=True)\n",
      "  )\n",
      "  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "123806c9-fd0e-4b96-952d-d6f2e7366554",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(vgg.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93424c11-789c-427f-8f50-ef5bc1d52324",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15190033-d69d-49db-96f0-ced438d25bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/1](20/196) || training loss 1.553 || training accuracy 45.45%\n",
      "Epoch[0/1](40/196) || training loss 0.8202 || training accuracy 70.72%\n",
      "Epoch[0/1](60/196) || training loss 0.6037 || training accuracy 79.24%\n",
      "Epoch[0/1](80/196) || training loss 0.5439 || training accuracy 81.43%\n",
      "Epoch[0/1](100/196) || training loss 0.4746 || training accuracy 83.38%\n",
      "Epoch[0/1](120/196) || training loss 0.4268 || training accuracy 84.73%\n",
      "Epoch[0/1](140/196) || training loss 0.4235 || training accuracy 85.49%\n",
      "Epoch[0/1](160/196) || training loss 0.3927 || training accuracy 86.60%\n",
      "Epoch[0/1](180/196) || training loss 0.3689 || training accuracy 87.52%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    vgg.train()\n",
    "    \n",
    "    loss_value = 0\n",
    "    matches = 0\n",
    "    for idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outs = vgg(images)\n",
    "        preds = torch.argmax(outs, dim=-1)\n",
    "        loss = criterion(outs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_value += loss.item()\n",
    "        matches += (preds == labels).sum().item()\n",
    "        \n",
    "        if (idx + 1) % log_interval == 0:\n",
    "            train_loss = loss_value / log_interval\n",
    "            train_acc = matches / batch_size / log_interval\n",
    "            print(\n",
    "                f\"Epoch[{epoch}/{epochs}]({idx + 1}/{len(train_loader)}) || \"\n",
    "                f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%}\"\n",
    "            )\n",
    "\n",
    "            loss_value = 0\n",
    "            matches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        vgg.eval()\n",
    "        \n",
    "        val_loss_items = []\n",
    "        val_acc_items = []\n",
    "        \n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outs = vgg(images)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "            loss = criterion(outs, labels)\n",
    "            \n",
    "            loss_item = loss.item()\n",
    "            acc_item = (preds==labels).sum().item()\n",
    "            \n",
    "            val_loss_items.append(loss_item)\n",
    "            val_acc_items.append(acc_item)\n",
    "            \n",
    "        val_loss = np.sum(val_loss_items) / len(test_loader)\n",
    "        val_acc = np.sum(val_acc_items) / len(test_loader.dataset)\n",
    "        \n",
    "        print(f\"epoch:[{epoch}/{epochs}] val_acc : {val_acc:4.2%}, val_loss: {val_loss:4.2} \")       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a92e865-c3ea-436f-81fd-b7b46ef453ec",
   "metadata": {},
   "source": [
    "# Gard-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdd9e8f-3109-4b08-bd24-ff3c4b8b3fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grad_cam(model, img, target_dir='./', index=0):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    gradients = model.get_activations_gradient()\n",
    "    \n",
    "    pooled_gradients = torch.mean(gradients, dim=[0,2,3])\n",
    "    \n",
    "    img = img.to(device)\n",
    "    \n",
    "    img = img.unsqueeze(0)\n",
    "    \n",
    "    activations = model.get_activations(img).detach()\n",
    "    \n",
    "    for i in range(img.size(1)):\n",
    "        activations[:,i,:,:] += pooled_gradients[i]\n",
    "\n",
    "    heatmap = torch.mean(activations, dim=1).squeeze().cpu()\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= torch.max(heatmap) \n",
    "    \n",
    "    img = img[0].cpu().permute(1, 2, 0).numpy()\n",
    "    resized_heatmap = heatmap.numpy()\n",
    "    resized_heatmap = cv2.resize(resized_heatmap, (img.shape[1], img.shape[0]))\n",
    "    resized_heatmap = np.uint8(255 * resized_heatmap)\n",
    "    resized_heatmap = cv2.applyColorMap(resized_heatmap, cv2.COLORMAP_JET)\n",
    "    \n",
    "    cv2.imwrite(os.path.join(target_dir, f'heatmap_{index}.jpg'), resized_heatmap)\n",
    "    \n",
    "    img = np.uint8(255 * img)\n",
    "    superimposed_img = (resized_heatmap) * 0.4 + img\n",
    "    \n",
    "    cv2.imwrite(os.path.join(target_dir, f'original_{index}.jpg'), img)\n",
    "    cv2.imwrite(os.path.join(target_dir, f'blending_{index}.jpg'), superimposed_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee99696a-857b-4476-b5e3-79abced9ce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, _ = next(iter(test_loader))\n",
    "\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b44fcec-ee9a-4b6b-9bdd-d80371861665",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_grad_cam(vgg, images[3], target_dir, index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683e0ad7-f7fa-4787-959b-37e175c27b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_img = cv2.imread(os.path.join(target_dir, 'original_0.jpg'))\n",
    "plt.imshow(original_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fd0b2a-b95e-45df-88c8-e41848741599",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_img = cv2.imread(os.path.join(target_dir, 'heatmap_0.jpg'))\n",
    "plt.imshow(heatmap_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b392e59-4f8a-43f4-84d0-e6963f39fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "blending_img = cv2.imread(os.path.join(target_dir, 'blending_0.jpg'))\n",
    "plt.imshow(blending_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e2f940-8ab1-451e-9e97-5e5efeaa9bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
